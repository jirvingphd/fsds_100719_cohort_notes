{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Trd6RcQwP-m8"
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:21.650842Z",
     "start_time": "2020-02-13T16:03:14.745924Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pillow\n",
    "!pip install opencv-contrib-python\n",
    "!pip install -U fsds_100719\n",
    "\n",
    "\n",
    "from fsds_100719.imports import *\n",
    "import os,glob\n",
    "os.path.abspath(os.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:22.790837Z",
     "start_time": "2020-02-13T16:03:21.652166Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:22.795516Z",
     "start_time": "2020-02-13T16:03:22.792300Z"
    }
   },
   "outputs": [],
   "source": [
    "## dataset\n",
    "base_folder = '/Users/jamesirving/Google Drive/Datasets/dogs-vs-cats-sorted/'\n",
    "os.listdir(base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:22.799532Z",
     "start_time": "2020-02-13T16:03:22.796894Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## DOG VS CAT\n",
    "train_base_dir = base_folder+'training_set/' #r'My Drive/Datasets/dogs-vs-cats/train'\n",
    "test_base_dir =base_folder+'test_set//' #r'My Drive/Datasets/dogs-vs-cats/test1'\n",
    "\n",
    "train_dogs = train_base_dir+'dogs/'\n",
    "train_cats = train_base_dir+'cats/'\n",
    "\n",
    "test_dogs = test_base_dir+'dogs/'\n",
    "test_cats = test_base_dir+'cats/'\n",
    "# os.listdir(test_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:22.802724Z",
     "start_time": "2020-02-13T16:03:22.800885Z"
    }
   },
   "outputs": [],
   "source": [
    "# files = glob.glob(train_dogcat_dir+'cats/*.jpg',recursive=True)\n",
    "# # files[:10]\n",
    "# # os.listdir(train_dogcat_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:22.834047Z",
     "start_time": "2020-02-13T16:03:22.804471Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2,glob,os\n",
    "dog_filenames = glob.glob(train_dogs+'*.jpg')\n",
    "cat_filenames = glob.glob(train_cats+'*.jpg')\n",
    "img_filenames = [*dog_filenames,*cat_filenames]\n",
    "print(len(img_filenames))\n",
    "img_filenames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:22.838772Z",
     "start_time": "2020-02-13T16:03:22.835323Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_image_cv2(filename, RGB=True):\n",
    "    \"\"\"Loads image using cv2 and converts to either matplotlib-RGB (default)\n",
    "    or grayscale.\"\"\"\n",
    "    import cv2\n",
    "\n",
    "    IMG = cv2.imread(filename)\n",
    "    if RGB: cmap = cv2.COLOR_BGR2RGB\n",
    "    else: cmap=cv2.COLOR_BGR2GRAY\n",
    "    return cv2.cvtColor(IMG,cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:23.212781Z",
     "start_time": "2020-02-13T16:03:22.840370Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load in and display image.\n",
    "IMG = load_image_cv2(img_filenames[0],RGB=False)\n",
    "\n",
    "## Even if you import as grayscale, must tell plt to use gray cmap\n",
    "fig,ax= plt.subplots(ncols=2,figsize=(12,5))\n",
    "ax[0].imshow(IMG)\n",
    "ax[1].imshow(IMG,cmap='gray')\n",
    "\n",
    "## Remove axes labels https://stackoverflow.com/a/2176591\n",
    "[(a.get_xaxis().set_visible(False), a.get_yaxis().set_visible(False)) for a in ax]\n",
    "print(IMG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:24.473578Z",
     "start_time": "2020-02-13T16:03:24.285231Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using seaborn color palette with imshow\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(sns.color_palette('RdBu',n_colors=25))\n",
    "plt.imshow(IMG,cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:25.124246Z",
     "start_time": "2020-02-13T16:03:24.916743Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(IMG,cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:25.765595Z",
     "start_time": "2020-02-13T16:03:25.620834Z"
    }
   },
   "outputs": [],
   "source": [
    "## RESIZING IMAGES\n",
    "print(IMG.shape)\n",
    "small = cv2.resize(IMG,(100,50))\n",
    "plt.imshow(small,cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:26.448264Z",
     "start_time": "2020-02-13T16:03:26.281030Z"
    }
   },
   "outputs": [],
   "source": [
    "## Resizing Using Ratios \n",
    "w_ratio = 0.5\n",
    "h_ratio = 0.5\n",
    "\n",
    "## Must Pass cv2.resize(IMG, (0,0) IMG, w_ratio,h_ratio)\n",
    "new_img = cv2.resize(IMG, (0,0), IMG, w_ratio,h_ratio)\n",
    "plt.imshow(new_img,cmap=cmap)\n",
    "print(new_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:27.215678Z",
     "start_time": "2020-02-13T16:03:27.050966Z"
    }
   },
   "outputs": [],
   "source": [
    "new_img = cv2.flip(new_img,0)\n",
    "plt.imshow(new_img,cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:27.668532Z",
     "start_time": "2020-02-13T16:03:27.665064Z"
    }
   },
   "outputs": [],
   "source": [
    "*a,_=test_base_dir.split('/')\n",
    "save_dir = '/'.join(a)\n",
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:28.306237Z",
     "start_time": "2020-02-13T16:03:28.302326Z"
    }
   },
   "outputs": [],
   "source": [
    "cv2.imwrite(save_dir+'example_save.jpg',new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:04:39.326801Z",
     "start_time": "2020-02-13T16:04:39.151576Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(cv2.imread(save_dir+'example_save.jpg',cv2.COLOR_BGR2RGB),cmap=cmap)\n",
    "plt.gcf().patch.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:04:39.769218Z",
     "start_time": "2020-02-13T16:04:39.765415Z"
    }
   },
   "outputs": [],
   "source": [
    "print(base_folder)\n",
    "os.listdir(base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:11.019925Z",
     "start_time": "2020-02-13T16:28:10.679367Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create training and test data\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(base_folder+'training_set/',\n",
    "                                                 target_size = (150, 150),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(base_folder+'test_set/',\n",
    "                                            target_size = (150, 150),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:11.255248Z",
     "start_time": "2020-02-13T16:28:11.023416Z"
    }
   },
   "outputs": [],
   "source": [
    "training_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:11.747290Z",
     "start_time": "2020-02-13T16:28:11.256861Z"
    }
   },
   "outputs": [],
   "source": [
    "shapes = [\"Batchsize\", \"img_width\",\"img_height\",\"img_dim\"]\n",
    "SHAPES = dict(zip(shapes, training_set[0][0].shape))\n",
    "print(SHAPES)\n",
    "print(training_set[0][0].shape)\n",
    "print('\\nLabels for batch')\n",
    "print(training_set[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:18.120169Z",
     "start_time": "2020-02-13T16:28:17.985574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Part 1 - Building the CNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(SHAPES['Batchsize'], (3, 3),\n",
    "                             input_shape = (SHAPES['img_width'], SHAPES['img_height'], SHAPES['img_dim']),\n",
    "                             activation = 'relu'))\n",
    "\n",
    "classifier.add(Conv2D(SHAPES['Batchsize'], (3, 3),\n",
    "                      input_shape = (SHAPES['img_width'], SHAPES['img_height'], SHAPES['img_dim']), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(SHAPES['Batchsize'], (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = SHAPES['Batchsize'], activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "# classifier.fit_generator(training_set,\n",
    "#                          steps_per_epoch = 2000,\n",
    "#                          epochs = 2,\n",
    "#                          validation_data = test_set,\n",
    "#                          validation_steps = 500,workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:18.755545Z",
     "start_time": "2020-02-13T16:28:18.753331Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/risingdeveloper/transfer-learning-in-keras-on-dogs-vs-cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:41.130469Z",
     "start_time": "2020-02-13T16:28:19.339734Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import InceptionResNetV2\n",
    "\n",
    "conv_base = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(150,150,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:41.303392Z",
     "start_time": "2020-02-13T16:28:41.132620Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:48.792947Z",
     "start_time": "2020-02-13T16:28:41.304826Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  #Sigmoid function at the end because we have just two classes\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:48.798283Z",
     "start_time": "2020-02-13T16:28:48.794242Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of trainable weights before freezing the conv base:', len(model.trainable_weights))\n",
    "conv_base.trainable = False\n",
    "print('Number of trainable weights after freezing the conv base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:48.852453Z",
     "start_time": "2020-02-13T16:28:48.799636Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=2e-5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:48.856533Z",
     "start_time": "2020-02-13T16:28:48.853517Z"
    }
   },
   "outputs": [],
   "source": [
    "len(training_set)*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:42:17.733053Z",
     "start_time": "2020-02-13T16:32:15.737807Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(training_set,\n",
    "                              steps_per_epoch = 500,\n",
    "                              epochs = 2, validation_data = test_set,\n",
    "                              validation_steps = 50,workers=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TN2YFPhxYm5h"
   },
   "source": [
    "## Overview - CNNs\n",
    "\n",
    "CNNs have certain features that identify patterns in images because of  \"convolution operation\" including:\n",
    "\n",
    "- Dense layers learn global patterns in their input feature space\n",
    "\n",
    "- Convolution layers learn local patterns, and this leads to the following interesting features:\n",
    "    - Unlike dense networks, local patterns recognized can be recognized _anywhere_ in the images. \n",
    "    - Deeper convolutional neural networks can learn spatial hierarchies.\n",
    "        - A first layer will learn small local patterns, a second layer will learn larger patterns using features of the first layer patterns, etc. \n",
    "    \n",
    "- Because of these properties, CNNs are great for tasks like:\n",
    "    - Image classification\n",
    "    - Object detection in images\n",
    "    - Picture neural style transfer\n",
    "\n",
    "## How Convolution works:\n",
    "\n",
    " <img src =\"https://raw.githubusercontent.com/jirvingphd/dsc-04-43-02-convolutional-neural-networks-online-ds-ft-021119/master/architecture-cnn.png\">\n",
    " \n",
    " - **The Convolution Operation:**\n",
    "    - Detect complex biilding blocks or features that can aid in a larger task. \n",
    "        - i.e. detecting vertical or horizontal edges (example below)<br>\n",
    "        <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-43-02-convolutional-neural-networks-online-ds-ft-021119/master/conv.png\" width=200><br>\n",
    "        - Here, the 3 x 3 matrix on the right represents the _filter_  used to perform a convolution operation to detect horizntal edges.\n",
    "        - The 5x5 image on the left  would by intensity values between 1-255(or rescaled to 1-10)\n",
    "    - **In Keras this is perfomed by a `Conv2d` layer.**\n",
    "        - Applies the convolution filter to every possible 3x3 region posible (for above example) <br>\n",
    " <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-43-02-convolutional-neural-networks-online-ds-ft-021119/master/convolution-layer-a.png\" width=400>\n",
    "\n",
    "### **Padding:**\n",
    "- Some issues arise using filteres on images.\n",
    "    - Image shrinks with each convolution layer.\n",
    "        - Examples:\n",
    "            - Start with a 5x5 image and use a 3x3 convolution matrix, result is 3x3 image. \n",
    "            - Start with a 10x10 image and use a 3x3 conv matrix, result is 8x8 image. \n",
    "    - The pixels around the edges are used much less in the outputs due to the filter. \n",
    "- Padding solves these issues \n",
    "    - Adding one layer of pixels around the edges preserves the image size with a 3x3 filter. \n",
    "    - Example: padding our 5x5 image (so it becomes a 6x6 image), we can now use a 3x3 conv filter and the resulting image is stil 5x5.\n",
    "- Padding terminology:\n",
    "    - 'Valid' - no padding.\n",
    "    - 'Same' - padding such that the output is the same size. \n",
    "\n",
    "- **Strided convolutions**\n",
    "    - Stride = how the filter moves over the image. \n",
    "        - Increasing stride so that it moves 2 pixels over instead of 1 results in a smaller # of locations.\n",
    "        - Rarely used in practice, but important option for some models. \n",
    " \n",
    "### Convolutions with color images: \n",
    "- Let's assume a 7x7 RGB, which becomes a 7 x 7 x 3 tensor. \n",
    "    - Will need to use a filter that has the third dimension as 3. \n",
    "    - Can use to detect features on individual color channels.\n",
    "- Can deconvolve with several 3D filters, then stack the output results together. \n",
    "\n",
    "- **Advantage: your image can be huge and the amount of parameters only depends on how many _filters_ used.**\n",
    "    - Lets say we have 20 3x3x3(color) images \n",
    "        - Would have $20 * 27 + a_bias_for_each_filter (1*20) = 560$ parameters\n",
    "\n",
    "- **Notation:**\n",
    "\n",
    "    - $f^{[l]}$ = size of the filter\n",
    "    - $p^{[l]}$ = padding\n",
    "    - $s^{[l]}$ = amount of stride\n",
    "    - $ n_c^{[l]}$ = number of filters\n",
    "    - filter: $f^{[l]}$ x $f^{[l]}$ x $ n_c^{[l-1]}$\n",
    "\n",
    "\n",
    "- Input =  $n_h^{[l-1]} * n_w^{[l-1]} * n_c^{[l-1]}$\n",
    "- Output = $n_h^{[l]} * n_w^{[l]} * n_c^{[l]}$\n",
    "\n",
    "- Height and width are given by:\n",
    "\n",
    "    - $n_h^{[l]}= \\Bigr\\lfloor\\dfrac{n_h^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1\\Bigr\\rfloor$\n",
    "\n",
    "    - $n_w^{[l]}= \\Bigr\\lfloor\\dfrac{n_w^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1\\Bigr\\rfloor$\n",
    "\n",
    "\n",
    "- Activations: $a^{[l]}$ is of dimension $ n_h^{[l]} * n_w^{[l]} * n_c^{[l]} $\n",
    "\n",
    "## Pooling Layers\n",
    "- After Conv layers, intended to significantly downsample the previous convolution layer.\n",
    "    - After conv layers detect edges/patterns, poooling layers will take a summary of the convoltuions from a larger section.\n",
    "    - Most common pooling layer is MaxPooling\n",
    "        - Takes the maximum of all convolutions from a larger area of the iamge. \n",
    "        - Works better than average pooling.\n",
    "- Downsampling is essential for viable executin times. \n",
    "\n",
    "- **MaxPooling Hyperparameters:**\n",
    "    - Parameter names:\n",
    "        - filter_size (f)\n",
    "        - stride (S)\n",
    "    - Common combinations:\n",
    "        - f=2, s=2\n",
    "        - f=3, s=2\n",
    "        \n",
    "### Additional Resources\n",
    "\n",
    "* https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n",
    "* https://datascience.stackexchange.com/questions/16463/what-is-are-the-default-filters-used-by-keras-convolution2d\n",
    "* https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\n",
    "* https://www.coursera.org/learn/convolutional-neural-networks/lecture/A9lXL/simple-convolutional-network-example\n",
    "* https://www.coursera.org/learn/convolutional-neural-networks/lecture/uRYL1/cnn-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:02:23.842498Z",
     "start_time": "2020-02-13T16:01:31.430Z"
    }
   },
   "outputs": [],
   "source": [
    "# from keras import layers\n",
    "# from keras import models\n",
    "# from keras import optimizers\n",
    "# import datetime\n",
    "\n",
    "# original_start = datetime.datetime.now()\n",
    "# start = datetime.datetime.now()\n",
    "\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "#                         input_shape=(150, 150, 3)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.Dense(128, activation='relu'))\n",
    "# model.add(layers.Dense(256, activation='relu'))\n",
    "# model.add(layers.Dense(512, activation='relu'))\n",
    "# model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "#               metrics=['acc'])\n",
    "\n",
    "# history = model.fit_generator(train_generator,\n",
    "#                               steps_per_epoch=100,\n",
    "#                               epochs=10,\n",
    "#                               validation_data=validation_generator,\n",
    "#                               validation_steps=50)\n",
    "\n",
    "# end = datetime.datetime.now()\n",
    "# elapsed = end - start\n",
    "# print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srbjybjqQ5m9"
   },
   "source": [
    "### Using os, shutil to create directories and copy files\n",
    "- from [Convolutional Neural Networks - Codealong](https://github.com/jirvingphd/dsc-04-43-03-convolutional-neural-networks-code-along-online-ds-ft-021119)\n",
    "\n",
    "- **first define the folders that currently contain the images get their filenames**\n",
    "\n",
    "```python\n",
    "import os, shutil\n",
    "\n",
    "# Define directories to be created:\n",
    "data_santa_dir = 'data/santa/'\n",
    "data_not_santa_dir = 'data/not_santa/'\n",
    "new_dir = 'split/'\n",
    "\n",
    "# Store the list of all the relevant training target images\n",
    "imgs_santa = [file for file in os.listdir(data_santa_dir) if file.endswith('.jpg')]\n",
    "print('There are',len(imgs_santa), 'santa images')\n",
    "\n",
    "# Store the list of all non-target images\n",
    "imgs_not_santa = [file for file in os.listdir(data_not_santa_dir) if file.endswith('.jpg')]\n",
    "print('There are', len(imgs_not_santa), 'images without santa')\n",
    "\n",
    "```\n",
    "\n",
    "- **Now create new directries and for training, testing, and validation images.**\n",
    "\n",
    "```python\n",
    "# Create the main folder for all of the new sub-folders\n",
    "os.mkdir(new_dir)\n",
    "\n",
    "# Create valid pathnames inside of the new_dir for training images\n",
    "train_folder = os.path.join(new_dir, 'train')\n",
    "train_santa = os.path.join(train_folder, 'santa')\n",
    "train_not_santa = os.path.join(train_folder, 'not_santa')\n",
    "\n",
    "# Create valid pathnames inside of the new_dir for testing images\n",
    "test_folder = os.path.join(new_dir, 'test')\n",
    "test_santa = os.path.join(test_folder, 'santa')\n",
    "test_not_santa = os.path.join(test_folder, 'not_santa')\n",
    "\n",
    "# Create valid pathnames inside of the new_dir for validation images\n",
    "val_folder = os.path.join(new_dir, 'validation')\n",
    "val_santa = os.path.join(val_folder, 'santa')\n",
    "val_not_santa = os.path.join(val_folder, 'not_santa')\n",
    "\n",
    "\n",
    "# Now create all of the folders defined above\n",
    "os.mkdir(test_folder)\n",
    "os.mkdir(test_santa)\n",
    "os.mkdir(test_not_santa)\n",
    "\n",
    "os.mkdir(train_folder)\n",
    "os.mkdir(train_santa)\n",
    "os.mkdir(train_not_santa)\n",
    "\n",
    "os.mkdir(val_folder)\n",
    "os.mkdir(val_santa)\n",
    "os.mkdir(val_not_santa)\n",
    "\n",
    "```\n",
    "\n",
    "- **Now that we have the folders, copy the desired # of images to the correct dataset folders**\n",
    "\n",
    "```python\n",
    "# The user decided to put 271 images in the training set, 100 in the validation set, and 90 in the test set\n",
    "# train santa\n",
    "imgs = imgs_santa[:271]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(train_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# validation santa\n",
    "imgs = imgs_santa[271:371]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(val_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# test santa\n",
    "imgs = imgs_santa[371:]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(test_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "## REPEATED FOR FOR THE NON-SANTA IMAGES - NOT SHOWN   \n",
    "```\n",
    "\n",
    "- Now that we have images in separate directories, we can use the Kera's ImageDataGenerators .flow_from_directory() method.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size=(64, 64), batch_size = 180) \n",
    "# ...do the same for train and val (not shown)\n",
    "\n",
    "\n",
    "# create the data sets\n",
    "train_images, train_labels = next(train_generator)\n",
    "\n",
    "# Make sure things worked\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "\n",
    "\n",
    "# Reshape the training images to have one column(/row?) for each image\n",
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "print(train_img.shape)\n",
    "\n",
    "\n",
    "# Reshape the labels to match the data\n",
    "train_y = np.reshape(train_labels[:,0], (542,1))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUgyK0HRQBxT"
   },
   "source": [
    "## Building CNN From Scratch Lab\n",
    "- https://github.com/learn-co-students/dsc-04-43-04-building-a-cnn-from-scratch-online-ds-ft-021119/tree/solution\n",
    "- CNN's are great for image processing\n",
    "### Image Data\n",
    "\n",
    "```python\n",
    "import os #for listdir()\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_dir = 'chest_xray_downsampled/train'\n",
    "validation_dir = 'chest_xray_downsampled/val/'\n",
    "test_dir = 'chest_xray_downsampled/test/'\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Train_generator example\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "```\n",
    "- **Images are store in ImageDataGenerators**\n",
    "    - Generally rescale to... intensity values? of 1./255\n",
    "    - load in files with ImageDataGenerator.flow_from_directory( :\n",
    "        - directory\n",
    "        - the target_size (the size to convert all images to)\n",
    "        - batch_size\n",
    "        - class_mode \n",
    "            - Note: selection of loss function determines chocie.\n",
    "                - If using 'binary_crossentropy' for binary classification, use class_mode='binary'\n",
    "                \n",
    "- ImageDataGenerators are also used for augmenting data. \n",
    "\n",
    "```python\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "```\n",
    "\n",
    "### Setting Up Initial Network\n",
    "```python\n",
    "from keras import models, layers, optimizers\n",
    "# or if want to do exact layers:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense)\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Initialize sequential model\n",
    "model = Sequential()\n",
    "```\n",
    "**1A) A CNN should start with a Conv2D later**\n",
    "\n",
    "```python\n",
    "layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "- Conv2D layers parameters (to change):\n",
    "    - filters:  # of samples to take from each image (kind of like # of neurons?) (e.g. filters=32)\n",
    "    - kernel_size: size (in pixels) of the filters (e.g kernel_size=(3,3)\n",
    "    - activation: activation function to use (e.g. 'relu')\n",
    "        \n",
    "**1B) MaxPooling2D layers following Conv2D layers**\n",
    "```python\n",
    "layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
    "```\n",
    "\n",
    "-  MaxPooling2D parameters:\n",
    "    -  pool_size: factor by which to downscale. \n",
    "        - e.g.pool_size=(2,2) will half information in vertical and horizational direction\n",
    "     \n",
    "**1C,optional) Add a Dropout layer to avoid overfitting:** [Udemy course suggestion]\n",
    "```python\n",
    "layers.Dropout(rate, noise_shape=None, seed=None)\n",
    "```\n",
    "- Dropout parameters:\n",
    "    - rate = 0.25 (used by udemy course)\n",
    "\n",
    "**2) Repeat: Continue layering combinations of Conv2D / MaxPooling2D layers** (Dropout too?):\n",
    "- Later layers will need larger # of filters to detect more abstract patterns.\n",
    "\n",
    "```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "```\n",
    "\n",
    "**3A) Flatten Data Before Passing on to Dense Layers for Classification/Learning**\n",
    "```python\n",
    "layers.Flatten(data_format =None)\n",
    "```\n",
    "**3B)  Add Dense layers at the end of the convolutional base for learning:**\n",
    "```python\n",
    "layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "```\n",
    "- Will only need to worry about basic parameters:\n",
    "    - Units:\n",
    "        - Larger #, used for the actual learning.\n",
    "    - Activation\n",
    "        - User choice, 'relu' is always good.\n",
    "        \n",
    "**3C) Add final Dense layer to determine output classification**\n",
    "- Add  a final small Dense layer (depending on number of classes?)\n",
    "    - For binary classification:\n",
    "        - units: 1\n",
    "        - activation: 'sigmoid'\n",
    "        \n",
    "``` python\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "**4) Compile the model, selecting loss function, optimizer, and metric**\n",
    "\n",
    "- Loss Function:\n",
    "    - For binary classifications, use 'binary_crossentropy'\n",
    "- Optimizer:\n",
    "    - Use RMSProp,\n",
    "        -Specify learning rate: ```lr = 1e-4```\n",
    "- Metrics:\n",
    "    - Use 'acc' for accuracy\n",
    "```python\n",
    "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Compile Model\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=optimizers.RMSProp(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "#Set the model to train; \n",
    "import datetime\n",
    "start=datetime.datetime.now()\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "\n",
    "end=datetime.datetime.now()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rz5t0lMYCfd"
   },
   "source": [
    "# Use Pretrained CNNs\n",
    "\n",
    "## Pretrained Networks Overview\n",
    "- Pretrained networks have already been trained on large pools of data, and have their weights frozen.\n",
    "    - They enable deep learning on fairly small image datasets\n",
    "    - a 'small' dataset is less than tens of thousands or hundreds of thousands of images\n",
    "\n",
    "- Pretrained networks can be used in whole or only specific parts, depending on your need/data.\n",
    "    - The shallower the layer of neurons, the more generic its features are. \n",
    "        - Therefore even if you data is very different, you can still use the lower layers for basic feature extraction.\n",
    "    - The deeper the layer, the more abstract its features are.\n",
    "        - so you may want to unfreeze the deeper/higher order classificaiton layers and re-train the network on your images. \n",
    "<br><br>\n",
    "### Where to find the pre-trained networks\n",
    "- **Pretrained Networks are available in [Keras.applications](https://keras.io/applications/)**\n",
    "    - This list of pretained models are for image classification. \n",
    "        - DenseNet\n",
    "        - InceptionResNetV2\n",
    "        - InceptionV3\n",
    "        - MobileNet\n",
    "        - NASNet\n",
    "        - ResNet50\n",
    "        - VGG16\n",
    "        - *VGG19* - used in labs\n",
    "        - Xception\n",
    "\n",
    "    - You can import these networks and use it as a function with 2 arguments:\n",
    "        1. `weights`\n",
    "            - Determines which data source's training data weights to use.\n",
    "            - ex:  `weights='imagenet'\n",
    "        2. `include_top`\n",
    "            - determines whetehr or not to include the fully-connected layer at the top of the network\n",
    "```python\n",
    "from keras.applications import MobileNet\n",
    "conv_base = MobileNet(weights='imagenet', include_top=True)\n",
    "```\n",
    "\n",
    "### How to use pretrained networks for feature extraction or for fine-tuning\n",
    "\n",
    "**You'll learn about two ways to use pre-trained networks:**\n",
    "- **Feature extraction**: here, you use the representations learned by a previous network to extract interesting features from new samples. \n",
    "    - Method 1) Use the convolutional base layers and run your data to detect the basic features, and save the output data, which is then run a new dense classifier, which is trained from scratch.  \n",
    "        - (+) It is fast\n",
    "        - (-) but cant use data augmentation. \n",
    "        - Note:  If your images are very different from the pretraining datasets, you may want to only use _part_ of the convolutional base but a _new_ densely connected classifier\n",
    "    - Method 2) Extend the conv_base by adding dense layers on top, running everything together. \n",
    "        - (+) allows for data sugmentation\n",
    "        - (-) extremely time-consuming and requires GPU\n",
    "  \n",
    "- **Fine-tuning**: when finetuning, you'll \"unfreeze\" a few top layers from the convolutional base of the model and train them again together with the densely connected classifier layers of the model. \n",
    "    - Note that you are changing the parts of the convolutional layers here that were used to detect the more abstract features.\n",
    "    - By doing this, you can make your model more relevant for the classification problem at hand.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "* http://cs231n.stanford.edu/syllabus.html\n",
    "* https://www.dlology.com/blog/gentle-guide-on-how-yolo-object-localization-works-with-keras/\n",
    "* https://www.dlology.com/blog/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "VHwZgQ9mQAu3",
    "outputId": "343aed57-15ae-4324-da54-43cc159db7e0"
   },
   "outputs": [],
   "source": [
    "from keras.applications import MobileNet\n",
    "conv_base = MobileNet(weights='imagenet',\n",
    "                  include_top = True)\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OStc-MxyrNMY"
   },
   "source": [
    "# Using  Pretrained Networks - Codealong\n",
    "## Theory/Tips\n",
    "\n",
    "## Code \n",
    "\n",
    "### Feature Extraction Method 1:\n",
    "\n",
    "```python\n",
    "from keras.applications import VGG19\n",
    "cnn_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(64, 64, 3))\n",
    "cnn_base.summary()\n",
    "\n",
    "# ---\n",
    "\n",
    "def extract_features(directory, sample_amount):\n",
    "    features = np.zeros(shape=(sample_amount, 2, 2, 512)) \n",
    "    labels = np.zeros(shape=(sample_amount))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory, target_size=(64, 64), \n",
    "        batch_size = 10, \n",
    "        class_mode='binary')\n",
    "    i=0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = cnn_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch \n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i = i + 1\n",
    "        if i * batch_size >= sample_amount:\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "# ---\n",
    "\n",
    "# you should be able to divide sample_amount by batch_size!!\n",
    "train_features, train_labels = extract_features(train_folder, 540) \n",
    "validation_features, validation_labels = extract_features(val_folder, 200) \n",
    "test_features, test_labels = extract_features(test_folder, 180)\n",
    "\n",
    "train_features = np.reshape(train_features, (540, 2 * 2 * 512))\n",
    "validation_features = np.reshape(validation_features, (200, 2 * 2 * 512))\n",
    "test_features = np.reshape(test_features, (180, 2 * 2 * 512))\n",
    "\n",
    "# ---\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=2 * 2 * 512))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(validation_features, validation_labels))\n",
    "\n",
    "results_test = model.evaluate(test_features, test_labels)\n",
    "\n",
    "# ---\n",
    "\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epch = range(1, len(train_acc) + 1)\n",
    "plt.plot(epch, train_acc, 'g.', label='Training Accuracy')\n",
    "plt.plot(epch, val_acc, 'g', label='Validation acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epch, train_loss, 'r.', label='Training loss')\n",
    "plt.plot(epch, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#---\n",
    "\n",
    "```\n",
    "\n",
    "## Feature Extraction Method 2\n",
    " - this method is much more costly, but allows us to use data augmentation\n",
    " \n",
    "- The process:\n",
    "    1. Add the pretrained model as the first layer\n",
    "    2. Add some dense layers as a classifier on top\n",
    "    3. Freeze the convolutional base\n",
    "        - This will prevent the weights from changing. \n",
    "        - The layer.trainable attribute indicates if a layer is frozen\n",
    "    4. Train the model. \n",
    "    \n",
    "    \n",
    "```python\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(cnn_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(132, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# ---\n",
    "\n",
    "#You can check whether a layer is trainable (or alter its setting) through the layer.trainable attribute:\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "#Similarly, we can check how many trainable weights are in the model:\n",
    "print(len(model.trainable_weights))\n",
    "\n",
    "# ---\n",
    "\n",
    "# Freeze the conv base\n",
    "cnn_base.trainable = False\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# get all the data in the directory split/train (542 images), and reshape them\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size= 20,\n",
    "        class_mode= 'binary') \n",
    "\n",
    "# get all the data in the directory split/validation (200 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size = 20,\n",
    "        class_mode= 'binary')\n",
    "\n",
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size = 180,\n",
    "        class_mode= 'binary')\n",
    "\n",
    "test_images, test_labels = next(test_generator)\n",
    "\n",
    "# ---\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# ---\n",
    "\n",
    "history = model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch= 27,\n",
    "              epochs = 10,\n",
    "              validation_data = val_generator,\n",
    "              validation_steps = 10)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0sjUefMuIs0"
   },
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ey5E6in_uGHO"
   },
   "source": [
    "Up till now, we have frozen the entire convolutional base. Again, it cannot be stressed enough how important this is before fine tuning the weights of the later layers of this base. Without training a classifier on the frozen base first, there will be too much noise in the model and initial epochs will overwrite any useful representations encoded in the pretrained model. That said, now that we have tuned a classifier to the frozen base, we can now unfreeze a few of the deeper layers from this base and further fine tune them to our problem scenario. In practice, this is apt to be particularly helpful where adapted models span new domain categories. For example, if the pretrained model is on cats and dogs and this is adapted to a problem specific to cats (a very relatively similar domain) there is apt to be little performance gain from fine tuning. On the other hand, if the problem domain is more substantially different, additional gains are more likely in adjusting these more abstract layers of the convolutional base. With that, let's take a look at how to unfreeze and fine tune these later layers.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "cnn_base.trainable = True\n",
    "\n",
    "# ---\n",
    "\n",
    "cnn_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in cnn_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        \n",
    "        \n",
    "# ---\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# ---\n",
    "history = model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch= 27,\n",
    "              epochs = 10,\n",
    "              validation_data = val_generator,\n",
    "              validation_steps = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtyhyKPOrgY5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkMVqngpLsKC"
   },
   "source": [
    "# NLP Content from Flation Data Science Bootcamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lReWoVYsTbwE"
   },
   "source": [
    "## Word Embeddings Lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBEb-dY-Lweq"
   },
   "source": [
    "- [Solution on Github](https://github.com/jirvingphd/dsc-04-45-04-generating-word-embeddings-lab-online-ds-ft-021119/tree/solution)\n",
    "\n",
    "- Use `nltk.word_tokenize` to tokenize new headlines data\n",
    "    - Can use `dataframe['column'].map(word_tokenize)` to tokenize a specific column in a df.\n",
    "    - After tokenization, leave in original order. \n",
    "    \n",
    "- Use `gensim.models.Word2Vec`\n",
    "    - [gensim website](https://radimrehurek.com/gensim/)\n",
    "    \n",
    "    \n",
    "- Instantiate a Word2Vec model:<br> `model = Word2Vec(data, size=100, window=5, min_count=1, workers=4)`\n",
    "    - `data` = text\n",
    "    - `size` =size of the embedding vectors to create\n",
    "    - `window` = # of words to include in sliding window\n",
    "    - `min_count` = number of times a word must appear to be counted\n",
    "    - `wokers` = number of threads to use during training\n",
    "    \n",
    "- `model.train(data, total_examples = model.corpus_count, epochs=10)`\n",
    "\n",
    "- Now can use the model.wv dictionary for methods\n",
    "    - `wv = model.wv`\n",
    "    \n",
    "    \n",
    "- **Get Word Similarity**\n",
    "    - Return the Most Similar Words \n",
    "        - `wv.most_similar('Texas')`\n",
    "         \n",
    "    - Return the Least Similar Words (not so meaningful)\n",
    "        - `wv.most_similar(negative='Texas')`\n",
    "\n",
    "- **To Get a Word's Vector**\n",
    "    - Use wv as a dictionary\n",
    "      - `wv['Texas']`\n",
    "      \n",
    "- **To Get All Word Vectors**\n",
    "    - `wv.vectors`\n",
    "        \n",
    "- **To Perform Word *Arithmetic***\n",
    "    - i.e. 'king' - 'man' + 'woman'\n",
    "    - Words to add should be `positive=`, words to subtract are `negative=`\n",
    "    - `wv.most_similar(positive=['king','woman'], negative=['man'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hs8MNYwNL7ka"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIiiqS1hMC_x"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "\n",
    "file = '/content/gdrive/My Drive/Colab Notebooks/datasets/News_Category_Dataset_v2.json'\n",
    "df = pd.read_json(file, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poAr1lwHOiZK"
   },
   "outputs": [],
   "source": [
    "# Concatenate description and headline.\n",
    "df['combined_text'] = df.headline+' '+df.short_description\n",
    "\n",
    "# Tokenize the combined_text column.\n",
    "data = df['combined_text'].map(word_tokenize)\n",
    "\n",
    "# Preview first 5\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "id": "XaQMDoG0Ox2y",
    "outputId": "95ff3fee-c532-46de-bb9a-d15a279a0c5b"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.train(data, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "\n",
    "wv = model.wv\n",
    "wv.most_similar('Texas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1pgxlGeTK7G"
   },
   "outputs": [],
   "source": [
    "wv.most_similar(negative='Texas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dOCWkeEmTRai"
   },
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['king','woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7QKTYb9OiOK"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5t5C_rbCTdd5"
   },
   "source": [
    "## Classification with Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QRaVtOWsTjVS"
   },
   "source": [
    "### Using Pretrained Word Vectors with GloVe\n",
    "- Best to load a top-tier industry-standard word mdels\n",
    "    - Most common is Global Vectors for Word Representation (GloVe) from the Stanford NLP Group.\n",
    "    - Loading in weights removes the need to instantiate a Word2Vec model. \n",
    "- **Instead, the process is:**\n",
    "    1. Get the total vocabulary of our data\n",
    "    2. Download and unzip the GloVe file needed from Standford NLP\n",
    "    3. Read the GloVe file, save only vectors for words in our dataset.\n",
    "    \n",
    "\n",
    "    \n",
    "- **File must be downloaded manually:**\n",
    "    - Must download the zip file for the Stanford Groups pretrained vectors [here.](https://nlp.stanford.edu/projects/glove/)(Lab used the smallest one (which is still 6 B words)\n",
    "    - Place the downloaded file directly into same folder as jupyter notebook.\n",
    "- **To use in python;**    \n",
    "    - First, must tokenize the words in the vocab as done above with nltk.\n",
    "    - Second, must turn the vocabulary into a `set`:\n",
    "        `total_vocabulary = set(word for headline in data for word in headline)`\n",
    "    - Third, load in the glove file, check for and keep only the words that are inside of total_vocabulary\n",
    "```python\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector\n",
    "```\n",
    "- The code above has created a dictionary called `glove, which contains all of the vectors for our data's vocabulary.\n",
    "\n",
    "### Mean Word Embeddings\n",
    "\n",
    "- Just loading in vectors does not describe sentence, only individual words.\n",
    "- To classify text, we need to calculate ***Mean Word Embeddings***. \n",
    "    - Simply get the vector for every word in a sentence and take the average.\n",
    "    - Mean Word vectors will always match the size of each individual word, no matter how many words appear in a sentence. \n",
    "    - Can easily put text into a form for Supervised Learning models, such as Support Vector Machines or Gradient Boosted Trees. \n",
    "    \n",
    "    \n",
    "### Coding a Custom Vectorizer Class (compatible with sklearn)\n",
    "\n",
    "### Deep Learning & Embedding Layers\n",
    "\n",
    "- Mean word embeddings lose some of the meaning, which is why **Sequence Models** exist. \n",
    "    - Recurrent Neural Networks \n",
    "    - Long Short Term Memory Cells\n",
    "- For deep learning, add **Embedding Layers** into the network.\n",
    "\n",
    "- **Embedding Layer Requirements**\n",
    "    - Learn the word embeddings for our data 'on the fly', get the benefits of Word2Vec without needing to train a Word2Vec model separately.\n",
    "    - Embedding Layers must always be the FIRST layer, immediately below the Input() layer.\n",
    "    - All words in the text must be integer-encoded (each word its own unique integer)\n",
    "    - Size of the embedding layer MUST be greater than the total vocabulary size\n",
    "        - First parameter denotes vocab size, the second parameter denotes word vector size. \n",
    "    - The size of sequences passed must be set when creating the layer. \n",
    "    \n",
    "###  Embedding Layers in Keras\n",
    "- [Classification with Word Embeddings - Lab ](https://github.com/learn-co-students/dsc-04-45-06-classification-with-word-embeddings-lab-online-ds-ft-021119)\n",
    "- [Keras Documentation](https://keras.io/layers/embeddings/)\n",
    "\n",
    "```python\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "y = pd.get_dummies(target).values\n",
    "```\n",
    "- **Preprocessing data for use with embedding layer:**\n",
    "    - Tokenize each example\n",
    "    - Convert to sequences\n",
    "    - Pad the sequences so same length\n",
    "\n",
    "```python\n",
    "tokenizer = text.Tokenizer(num_words=20000) # limiting to first 20000 words in vocab\n",
    "tokenizer.fit_on_texts(list(df.combined_text))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(df.combined_text)\n",
    "X_t = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)\n",
    "\n",
    "```\n",
    "\n",
    "- **Setting up the network architecture:**\n",
    "    - Input layer first\n",
    "    - Embedding layer second\n",
    "        - pass size of vocab, embedding_size\n",
    "        - embedding_size = 128\n",
    "    - LSTM layer third\n",
    "        - Followed by a GlobalMaxPooling1D layer\n",
    "        - Followed by  Dropout layer\n",
    "    - Dense layer for classification (activation='relu')\n",
    "        - Followed by another Dropout layer\n",
    "    - Final Dense layer\n",
    "        - Number of neurons = # of possible classes.\n",
    "        - activation = 'softmax'\n",
    "\n",
    "```python\n",
    "\n",
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = LSTM(25, return_sequences=True)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# There are 41 different possible classes, so we use 41 neurons in our output layer\n",
    "x = Dense(41, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_t, y, epochs=2, batch_size=32, validation_split=0.1)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Lab's W2vVectorizer Class (edited for bs_ds compatibility)\n",
    "- Original did not have import numpy statements and did not accept glove during \\_\\_init__ (but still expected it to be present)\n",
    "```python\n",
    "class W2vVectorizer(object):\n",
    "    \"\"\"From Learn.co Text Classification with Word Embeddings Lab.\n",
    "    An sklearn-comaptible class containing the vectors for the fit Word2Vec.\"\"\"\n",
    "\n",
    "    def __init__(self, w2v, glove):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        import numpy as np\n",
    "\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "\n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline.\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        import numpy as np\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])\n",
    "```\n",
    "\n",
    "#### With W2vVectorizer, can use in sklearn Pipelines:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "# ---\n",
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]\n",
    "# ---\n",
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]\n",
    "scores\n",
    "# ---\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "My Flatiron Bootcamp Notes - Mod 4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
